# -*- coding: utf-8 -*-
"""Q4 AI LAB TEST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11llPwbvMWidreY3FDftDzWACJuQwVzo7
"""

import streamlit as st
import nltk
from nltk.tokenize import sent_tokenize
from PyPDF2 import PdfReader
import pandas as pd

# -------------------------------
# Page Configuration
# -------------------------------
st.set_page_config(
    page_title="NLTK Text Chunking Tool",
    layout="wide"
)

st.title("üìÑ Text Chunking Web App")
st.caption("Sentence-level chunking from PDF using NLTK")

# -------------------------------
# Download NLTK resources (cached)
# -------------------------------
@st.cache_resource
def download_nltk_data():
    nltk.download("punkt")
    nltk.download("punkt_tab")

download_nltk_data()

# -------------------------------
# Step 1: Upload PDF file
# -------------------------------
uploaded_pdf = st.file_uploader(
    "Upload a PDF document",
    type=["pdf"]
)

if uploaded_pdf is not None:

    # -------------------------------
    # Step 2: Extract text from PDF
    # -------------------------------
    reader = PdfReader(uploaded_pdf)
    extracted_text = ""

    for page_num, page in enumerate(reader.pages):
        text = page.extract_text()
        if text:
            extracted_text += text + " "

    st.success("PDF text successfully extracted!")

    # -------------------------------
    # Step 3: Sentence tokenization
    # -------------------------------
    sentences = sent_tokenize(extracted_text)

    st.subheader("üß© Preview of Extracted Sentences (Index 10‚Äì20)")

    if len(sentences) >= 21:
        preview = sentences[10:21]
        for i, sentence in enumerate(preview, start=10):
            st.write(f"{i}: {sentence}")
    else:
        st.warning("The document does not contain enough sentences for preview.")

    # -------------------------------
    # Step 4: Display all sentence chunks
    # -------------------------------
    st.subheader("üîç Sentence Chunking Results")

    chunk_df = pd.DataFrame({
        "Sentence Index": range(len(sentences)),
        "Sentence Text": sentences
    })

    st.dataframe(chunk_df, use_container_width=True)

    st.info(
        "The NLTK sentence tokenizer splits large unstructured text into smaller, "
        "meaningful sentence chunks that are useful for semantic analysis, "
        "summarization, and NLP pipelines."
    )